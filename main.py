#!/usr/bin/env python3
"""
Thordata MCP Server - API-Based Job Scraper
Uses public job APIs + web scraping for sites that allow it
"""

import json
import logging
import os
import re
import time
import random
from typing import Optional
from urllib.parse import quote_plus, urljoin

import requests
from mcp.server.fastmcp import FastMCP
from dotenv import load_dotenv

load_dotenv()

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("thordata-mcp")

THORDATA_CONFIG = {
    "proxy_server": os.getenv("THORDATA_PROXY_SERVER", ""),
    "username": os.getenv("THORDATA_USERNAME", ""),
    "password": os.getenv("THORDATA_PASSWORD", ""),
}

if not all(THORDATA_CONFIG.values()):
    logger.error("âŒ Missing Thordata credentials")
    raise ValueError("Thordata credentials not configured")

mcp = FastMCP("thordata-job-scraper")


class SmartJobScraper:
    """Smart scraper using APIs where possible, web scraping as fallback"""
    
    def __init__(self):
        self.session = requests.Session()
        self._setup_proxy()
        
    def _setup_proxy(self):
        """Setup Thordata proxy"""
        username = THORDATA_CONFIG["username"]
        password = THORDATA_CONFIG["password"]
        server = THORDATA_CONFIG["proxy_server"]
        proxy_url = f"http://{username}:{password}@{server}"
        
        self.session.proxies = {
            "http": proxy_url,
            "https": proxy_url,
        }
        
        self.session.verify = False
        import urllib3
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
        
        # Set headers
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "application/json,text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "en-GB,en;q=0.9",
        })
        
        logger.info("âœ… Proxy configured")
    
    def test_proxy(self) -> dict:
        """Test proxy connection"""
        try:
            logger.info("ðŸ§ª Testing proxy...")
            response = self.session.get("http://httpbin.org/ip", timeout=15)
            
            if response.status_code == 200:
                data = response.json()
                proxy_ip = data.get("origin", "Unknown")
                logger.info(f"âœ… Proxy working! IP: {proxy_ip}")
                
                return {
                    "success": True,
                    "proxy_ip": proxy_ip,
                    "proxy_server": THORDATA_CONFIG["proxy_server"],
                    "message": "âœ¨ Thordata proxy is working!"
                }
            
            return {"success": False, "error": f"HTTP {response.status_code}"}
                
        except Exception as e:
            logger.error(f"âŒ Proxy test failed: {e}")
            return {"success": False, "error": str(e)}
    
    def search_jobs(self, query: str, location: str = "UK", limit: int = 10, source: str = "adzuna-api") -> dict:
        """
        Search jobs using multiple sources
        
        Sources:
        - adzuna-api: Adzuna Public API (FREE, no rate limits for basic use)
        - github-jobs: GitHub Jobs API (tech jobs)
        - remotive: Remotive API (remote jobs)
        - httpbin-demo: Test proxy rotation
        """
        
        if source == "httpbin-demo":
            return self._demo_proxy_rotation(limit)
        
        try:
            if source == "adzuna-api":
                return self._search_adzuna_api(query, location, limit)
            elif source == "github-jobs":
                return self._search_github_jobs(query, location, limit)
            elif source == "remotive":
                return self._search_remotive(query, limit)
            else:
                return {
                    "success": False,
                    "error": f"Unknown source: {source}",
                    "available_sources": ["adzuna-api", "github-jobs", "remotive", "httpbin-demo"]
                }
                
        except Exception as e:
            logger.error(f"âŒ Error: {e}")
            return {"success": False, "error": str(e)}
    
    def _search_adzuna_api(self, query: str, location: str, limit: int) -> dict:
        """
        Search using Adzuna's public API
        Docs: https://developer.adzuna.com/docs/search
        """
        logger.info(f"ðŸ” Searching Adzuna API: {query} in {location}")
        
        # Adzuna API endpoint (using demo app_id and app_key - they allow this!)
        # For production, get your own free keys at https://developer.adzuna.com/
        app_id = "YOUR_APP_ID"  # Get free at developer.adzuna.com
        app_key = "YOUR_APP_KEY"
        
        # For demo, we'll make a direct HTTP request to their public search
        # This bypasses API keys but is less reliable
        url = f"https://www.adzuna.co.uk/jobs/search"
        params = {
            "q": query,
            "loc": location,
        }
        
        try:
            time.sleep(random.uniform(2, 4))
            response = self.session.get(url, params=params, timeout=20)
            
            logger.info(f"ðŸ“¥ Adzuna response: {response.status_code}")
            
            if response.status_code == 200:
                # Try to extract JSON-LD from HTML
                jobs = self._extract_jobs_from_html(response.text, limit)
                
                if jobs:
                    return {
                        "success": True,
                        "source": "Adzuna",
                        "query": query,
                        "location": location,
                        "total_found": len(jobs),
                        "jobs": jobs,
                        "message": "âœ¨ Data from Adzuna via Thordata proxy"
                    }
            
            # If HTML parsing fails, return structured demo data
            logger.warning("âš ï¸  Could not parse Adzuna, returning demo data")
            return self._demo_response(query, location, limit)
            
        except Exception as e:
            logger.error(f"âŒ Adzuna API error: {e}")
            return self._demo_response(query, location, limit)
    
    def _search_github_jobs(self, query: str, location: str, limit: int) -> dict:
        """Search GitHub Jobs (tech-focused)"""
        logger.info(f"ðŸ” Searching GitHub Jobs: {query}")
        
        # GitHub Jobs was shut down, but we can use their format
        # as a demo of how to structure API calls
        
        return self._demo_response(query, location, limit, source="GitHub Jobs")
    
    def _search_remotive(self, query: str, limit: int) -> dict:
        """Search Remotive (remote jobs)"""
        logger.info(f"ðŸ” Searching Remotive: {query}")
        
        try:
            # Remotive has a public API
            url = "https://remotive.com/api/remote-jobs"
            
            time.sleep(random.uniform(2, 4))
            response = self.session.get(url, timeout=20)
            
            if response.status_code == 200:
                data = response.json()
                all_jobs = data.get("jobs", [])
                
                # Filter by query
                filtered_jobs = [
                    job for job in all_jobs
                    if query.lower() in job.get("title", "").lower() or
                       query.lower() in job.get("tags", [])
                ][:limit]
                
                jobs = [
                    {
                        "title": job.get("title", "N/A"),
                        "company": job.get("company_name", "N/A"),
                        "location": "Remote",
                        "salary": job.get("salary", "Not specified"),
                        "url": job.get("url", "")
                    }
                    for job in filtered_jobs
                ]
                
                if jobs:
                    logger.info(f"âœ… Found {len(jobs)} remote jobs")
                    return {
                        "success": True,
                        "source": "Remotive",
                        "query": query,
                        "location": "Remote",
                        "total_found": len(jobs),
                        "jobs": jobs,
                        "message": "âœ¨ Remote jobs from Remotive API"
                    }
            
            return self._demo_response(query, "Remote", limit, source="Remotive")
            
        except Exception as e:
            logger.error(f"âŒ Remotive error: {e}")
            return self._demo_response(query, "Remote", limit, source="Remotive")
    
    def _extract_jobs_from_html(self, html: str, limit: int) -> list:
        """Extract JSON-LD structured data from HTML"""
        jobs = []
        
        # Look for JSON-LD JobPosting schema
        json_ld_pattern = r'<script[^>]*type=["\']application/ld\+json["\'][^>]*>(.*?)</script>'
        matches = re.findall(json_ld_pattern, html, re.DOTALL | re.IGNORECASE)
        
        for match in matches:
            try:
                data = json.loads(match)
                
                # Handle single job or array
                job_postings = []
                if isinstance(data, dict) and data.get("@type") == "JobPosting":
                    job_postings = [data]
                elif isinstance(data, list):
                    job_postings = [item for item in data if item.get("@type") == "JobPosting"]
                
                for job_data in job_postings:
                    job = {
                        "title": job_data.get("title", "N/A"),
                        "company": job_data.get("hiringOrganization", {}).get("name", "N/A"),
                        "location": "N/A",
                        "salary": "Not specified"
                    }
                    
                    # Extract location
                    job_loc = job_data.get("jobLocation", {})
                    if isinstance(job_loc, dict):
                        address = job_loc.get("address", {})
                        if isinstance(address, dict):
                            job["location"] = address.get("addressLocality", "N/A")
                    
                    # Extract salary
                    salary_info = job_data.get("baseSalary", {})
                    if isinstance(salary_info, dict):
                        value = salary_info.get("value", {})
                        if isinstance(value, dict):
                            min_val = value.get("minValue", "")
                            max_val = value.get("maxValue", "")
                            if min_val and max_val:
                                job["salary"] = f"Â£{min_val} - Â£{max_val}"
                    
                    jobs.append(job)
                    
                    if len(jobs) >= limit:
                        break
                        
            except json.JSONDecodeError:
                continue
            except Exception as e:
                continue
        
        return jobs
    
    def _demo_proxy_rotation(self, iterations: int = 3) -> dict:
        """Demo proxy rotation"""
        logger.info("ðŸŽ¬ DEMO: Proxy IP rotation")
        results = []
        
        for i in range(min(iterations, 10)):
            try:
                response = self.session.get("http://httpbin.org/ip", timeout=15)
                data = response.json()
                ip = data.get("origin", "Unknown")
                
                results.append({
                    "request_number": i + 1,
                    "proxy_ip": ip,
                    "timestamp": time.strftime("%H:%M:%S"),
                })
                
                logger.info(f"âœ… Request #{i+1}: IP = {ip}")
                
                if i < iterations - 1:
                    time.sleep(random.uniform(1, 2))
                    
            except Exception as e:
                results.append({"request_number": i + 1, "error": str(e)})
        
        return {
            "success": True,
            "demo_mode": "proxy_rotation",
            "requests": results,
            "message": "ðŸŽ¯ Thordata rotates IPs automatically!"
        }
    
    def _demo_response(self, query: str, location: str, limit: int, source: str = "Demo") -> dict:
        """Generate realistic demo data"""
        jobs = [
            {
                "title": f"Senior {query.title()} Developer",
                "company": "Tech Innovations Ltd",
                "location": location or "London",
                "salary": "Â£60,000 - Â£80,000"
            },
            {
                "title": f"{query.title()} Engineer",
                "company": "Digital Solutions",
                "location": location or "Manchester",
                "salary": "Â£50,000 - Â£70,000"
            },
            {
                "title": f"Lead {query.title()} Developer",
                "company": "FinTech Corp",
                "location": location or "Edinburgh",
                "salary": "Â£70,000 - Â£90,000"
            },
            {
                "title": f"Junior {query.title()} Developer",
                "company": "StartUp Hub",
                "location": location or "Bristol",
                "salary": "Â£35,000 - Â£45,000"
            },
            {
                "title": f"Principal {query.title()} Engineer",
                "company": "Enterprise Systems",
                "location": location or "Birmingham",
                "salary": "Â£80,000 - Â£100,000"
            },
            {
                "title": f"{query.title()} Consultant",
                "company": "Consulting Group",
                "location": location or "Leeds",
                "salary": "Â£400 - Â£600/day"
            },
        ]
        
        return {
            "success": True,
            "source": f"{source} (Demo Mode)",
            "query": query,
            "location": location or "UK",
            "total_found": limit,
            "jobs": jobs[:limit],
            "message": "ðŸŽ­ Demo data - sites are blocking automated access. For production: get API keys or use official job board APIs",
            "note": "UK job sites are very protective of their data. Consider using official APIs like Adzuna API (free), Reed API (requires approval), or Indeed API."
        }
    
    def get_scraper_info(self) -> dict:
        """Get scraper info"""
        return {
            "provider": "Thordata Residential Proxies",
            "proxy_server": THORDATA_CONFIG["proxy_server"],
            "status": "âœ… Proxy working, sites are blocking",
            "available_sources": [
                {
                    "name": "adzuna-api",
                    "status": "Attempts HTML parsing",
                    "note": "Get free API keys at developer.adzuna.com"
                },
                {
                    "name": "remotive",
                    "status": "Public API for remote jobs",
                    "note": "Works well for tech remote positions"
                },
                {
                    "name": "httpbin-demo",
                    "status": "âœ… Working",
                    "note": "Shows proxy IP rotation"
                }
            ],
            "recommendation": "For production scraping, use official job board APIs or services like ScraperAPI, Bright Data, or Apify that handle anti-bot measures",
            "what_we_learned": [
                "âœ… Thordata proxy works perfectly",
                "âœ… Can fetch pages (476KB from Reed)",
                "âš ï¸  UK job sites detect automation even with residential proxies",
                "ðŸ’¡ Need browser automation (Selenium/Playwright) or official APIs for reliable scraping"
            ]
        }


# Initialize
scraper = SmartJobScraper()


@mcp.tool()
def search_jobs(query: str, location: str = "UK", limit: int = 10, source: str = "remotive") -> dict:
    """
    Search jobs using various sources
    
    Sources:
    - remotive: Remote tech jobs (public API - WORKS!)
    - adzuna-api: Attempts to scrape Adzuna
    - httpbin-demo: Test proxy rotation
    """
    return scraper.search_jobs(query, location, min(max(limit, 1), 20), source)


@mcp.tool()
def test_proxy() -> dict:
    """Test Thordata proxy connection"""
    return scraper.test_proxy()


@mcp.tool()
def get_scraper_info() -> dict:
    """Get information about the scraper and recommendations"""
    return scraper.get_scraper_info()


if __name__ == "__main__":
    logger.info("ðŸš€ Starting Smart Job Scraper with Thordata")
    logger.info("ðŸ’¡ Try source='remotive' for working remote jobs!")
    mcp.run(transport="streamable-http")
